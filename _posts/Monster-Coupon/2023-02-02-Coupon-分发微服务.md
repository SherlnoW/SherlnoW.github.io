---
layout: post
title: 【项目】Coupon-分发微服务
date: 2023-02-02 13:00
description: Coupon-分发微服务
tag:
- 项目
- Coupon
- 微服务
- Java
---

# Coupon-分发微服务

### 创建模块

```java
/**
 * <h1>分发系统的启动入口</h1>
 */
@EnableJpaAuditing      // 开启 Jpa 的审计功能
@EnableFeignClients     // 开启 Feign, 允许应用访问其他的微服务
@EnableCircuitBreaker   // 开启断路器
@EnableEurekaClient     // 标识当前的应用是 Eureka Client, 即需要向 Eureka Server 去注册
@SpringBootApplication  // 标识是 SpringBoot 应用
public class DistributionApplication {

    @Bean
    @LoadBalanced   // 开启负载均衡
    RestTemplate restTemplate() {
        return new RestTemplate();
    }

    public static void main(String[] args) {
        SpringApplication.run(DistributionApplication.class, args);
    }
}
```

### 功能代码

##### 服务功能接口定义

* **IKafkaService**：消费优惠券 Kafka 消息
* **IRedisService**：根据 userId 和状态找到缓存的优惠券列表数据、保存空的优惠券列表到缓存中、尝试从 Cache 中获取一个优惠券码、将优惠券保存到 Cache 中
* **IUserService**：根据用户 id 和状态查询优惠券记录、根据用户 id 查找当前可以领取的优惠券模板、用户领取优惠券、结算(核销)优惠券。

##### Redis 的使用

在分发服务中使用的 Redis 客户端都是 StringRedisTemplate，它是最常用的 Redis 客户端，
用于存取 key 和 value 都是字符串类型的数据。默认采用 String 的序列化策略（StringRedisSerializer）。

```java
public class StringRedisTemplate extends RedisTemplate<String, String> {

    /**
     * Constructs a new <code>StringRedisTemplate</code> instance. {@link #setConnectionFactory(RedisConnectionFactory)}
     * and {@link #afterPropertiesSet()} still need to be called.
     */
    public StringRedisTemplate() {
        RedisSerializer<String> stringSerializer = new StringRedisSerializer();
        setKeySerializer(stringSerializer);
        setValueSerializer(stringSerializer);
        setHashKeySerializer(stringSerializer);
        setHashValueSerializer(stringSerializer);
    }
  ...
}
```

> 这里一定需要注意，不要去使用 RedisTemplate。因为在使用 RedisTemplate 时，
> 通常需要自己去指定 key 和 value 的序列化器。如果没有指定，
> 则使用默认的 JdkSerializationRedisSerializer（JDK 的序列化策略）。

**总结**
* 操作 Redis 的数据结构时，使用 opsForXXX，这里的 XXX 就对应到 Redis 提供的数据结构，表达的意思是对 XXX 数据结构进行操作 
* 当需要存取的数据是字符串类型时（大多数情况下），使用 StringRedisTemplate 
* 当需要存取的数据是 Java Object 时，使用 RedisTemplate（需要定义好序列化策略） 
* 不需要使用 RedisConnection

##### Kafka 的使用

spring-kafka 将对 Kafka 的使用封装的十分简单。发送消息即 send、消费消息即 KafkaListener。

**Maven依赖**
```java
<!-- 需要注意 pom 版本要适应 Kafka 的版本 -->
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
    <version>2.2.0.RELEASE</version>
</dependency>
```

**配置文件**
```yaml
spring:
  kafka:
    # broker 列表，数组结构
    bootstrap-servers:
      - 127.0.0.1:9092
    consumer:
      # 消费者默认的 group id
      group-id: imooc-coupon-x
      # 消费者默认的消费策略，这里指的是从最新的位置开始消费，即不管之前的消息
      auto-offset-reset: latest
```

**发送Kafka消息**

首先，需要注入 KafkaTemplate<String, String>，两个泛型分别是 key 和 value 的类型。
key 用于做分区的 hash，value 则是 kafka 消息。之后，就可以通过 send 方法发送消息。

```java
public ListenableFuture<SendResult<K, V>> send(String topic, @Nullable V data) {
	ProducerRecord<K, V> producerRecord = new ProducerRecord<>(topic, data);
	return doSend(producerRecord);
}
```
对于这个 send 方法，可以发现：
* key 并不需要传递，即为 null，代表使用默认的消息分区策略 
* 是个异步方法，因为返回 Future 对象 
* 消费 Kafka 消息

**消费Kafka消息**

只需要在相应的处理方法上标注 @KafkaListener 注解即可，并在注解中指定相关的属性。

```java
public @interface KafkaListener {
  ......
  // 消费的 topic 数组
	String[] topics() default {};
	// 消费的 topic 正则表达式
	String topicPattern() default "";
	// 消费指定 topic 的 partition
	TopicPartition[] topicPartitions() default {};
  ......
	// 指定 group id，如果不指定，则使用 application 中配置的
	String groupId() default "";
  ......
}
```

### 数据一致性

详情见文章：[数据一致性](https://sherlnow.github.io/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/)

这里改为采用 Cache Aside 策略，也就是旁路缓存策略，是为了解决缓存和数据的一致问题。
该策略可以细分为读策略和写策略。
写策略：
* 更新数据库中的数据； 
* 删除缓存中的数据。
读策略： 
* 如果读取的数据命中了缓存，则直接返回数据； 
* 如果读取的数据没有命中缓存，则从数据库中读入数据，然后将数据写入缓存，并返回给用户。

缺点：

并不一定保证数据一致性

1. 读写并发场景下，首先来自线程 1 的读请求在未命中缓存的情况下查询数据库（step1），接着来自线程 2 的写请求更新数据库（step2），但由于一些极端原因，线程 1 中读请求的更新缓存操作晚于线程 2 中写请求的删除缓存的操作（step 4 晚于 step 3 ），那么这样便会导致最终写入缓存中的是来自线程1的旧值，而写入数据库中的是来自线程 2 的新值，即缓存落后于数据库，此时再有读请求命中缓存（step5），读取到的便是旧值。
这种场景的出现，不仅需要缓存失效且读写并发执行，而且还需要读请求查询数据库的执行早于写请求更新数据库，同时读请求的执行完成晚于写请求。足以见得，这种不一致场景产生的条件非常严格，在实际的生产中出现的可能性较小。
2. 在并发环境下，Cache-Aside中也存在读请求命中缓存的时间点在写请求更新数据库之后，删除缓存之前，这样也会导致读请求查询到的缓存落后于数据库的情况。虽然在下一次读请求中，缓存会被更新，
但如果业务层面对这种情况的容忍度较低，那么可以采用加锁在写请求中保证“更新数据库&删除缓存”的串行执行为原子性操作（同理也可对读请求中缓存的更新加锁）。加锁势必会导致吞吐量的下降，故采取加锁的方案应该对性能的损耗有所预期。或者可以使用事务：在更新优惠券状态之前，启动一个数据库事务，事务中先读取缓存，如果命中则删除缓存，然后执行写操作，更新数据库，最后提交事务。这个过程中，其他的读请求会被阻塞，直到该事务提交完成。这样可以确保读请求不会在写请求更新数据库之前命中缓存，从而保证数据的一致性。如果写操作失败，事务会自动回滚，缓存也不会被删除，从而保证缓存中的数据与数据库保持一致。使用事务会对性能造成一定的影响。

